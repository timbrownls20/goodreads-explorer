# Goodreads Explorer

Multi-component application for scraping, exploring, and analyzing Goodreads library data.

**Features**:
- ðŸ“¥ Scrape Goodreads library data from profile URLs
- ðŸ“Š Visualize reading statistics with interactive dashboard
- ðŸ“ˆ Analyze reading trends, ratings, and patterns
- ðŸ” Explore book metadata (genres, authors, publication dates)

## Project Structure

This is a multi-component monorepo with separate codebases for different concerns:

```
goodreads-explorer/
â”œâ”€â”€ parser/             # Python scraping & parsing (Feature 001)
â”‚   â”œâ”€â”€ src/           # Python source (models, parsers, scrapers, exporters, CLI)
â”‚   â”œâ”€â”€ tests/         # Python test suite
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ dashboard-ui/       # React + TypeScript + Vite SPA (Feature 002)
â”œâ”€â”€ dashboard-backend/  # NestJS + TypeORM API server (Feature 002)
â”œâ”€â”€ database/          # PostgreSQL setup & migrations (Feature 002)
â”œâ”€â”€ docker-compose.yml # Docker Compose for dashboard deployment
â””â”€â”€ specs/             # Feature specifications & implementation plans
```

### Components

#### 1. Scraper (Feature 001) - Python
- Scrapes Goodreads library data from profile URLs
- Parses HTML with BeautifulSoup4
- Exports to JSON (individual files per book) or CSV
- Provides CLI: `goodreads-explorer scrape --user-id USER_ID`
- **Status**: âœ… Complete (MVP)

**Documentation**: [Parse Component README](./parser/README.md) | [Quickstart Guide](./specs/001-scrape-goodreads-library/quickstart.md)

#### 2. Analytics Dashboard (Feature 002) - Full Stack
- Full-stack web application for visualizing library data
- Upload JSON files from scraper â†’ View analytics
- Summary statistics (totals, ratings, reading pace, year-over-year)
- Rating distribution visualization
- Session-based user tracking
- **Status**: âœ… Phase 3 Complete (MVP: Upload & Summary Statistics)
- **Planned**: Phase 4-6 (Trends, Categories, Filtering)

**Documentation**: [Dashboard README](./DASHBOARD.md) | [Full Spec](./specs/002-analytics-dashboard/spec.md)

**Quick Start**:
```bash
cp .env.example .env
docker-compose up -d
# Open http://localhost:3000
```

## Technology Stack

### Scraper (Python)
- **Runtime**: Python 3.10+ (3.12 recommended)
- **Libraries**: BeautifulSoup4, httpx, Pydantic v2
- **Testing**: pytest

### Dashboard (Full Stack)
- **Frontend**: React 18 + TypeScript + Vite
- **Backend**: Node.js 20 + NestJS 10 + TypeORM
- **Database**: PostgreSQL 15+
- **Deployment**: Docker Compose (3 containers)

## Getting Started

### Option 1: Scrape Your Goodreads Library

```bash
# Install scraper
cd parser
python3 -m pip install -e .

# Scrape library data
goodreads-explorer scrape --user-id YOUR_GOODREADS_USER_ID

# Output: JSON files in timestamped directory
```

**Documentation**: [Scraper Quickstart](./specs/001-scrape-goodreads-library/quickstart.md)

### Option 2: Run Analytics Dashboard

```bash
# Start dashboard (requires Docker)
cp .env.example .env
docker-compose up -d

# Open dashboard
open http://localhost:3000

# Upload JSON files from scraper
# View analytics automatically
```

**Documentation**: [Dashboard README](./DASHBOARD.md) | [Dashboard Quickstart](./specs/002-analytics-dashboard/quickstart.md)

### Full Workflow

1. **Scrape**: `goodreads-explorer scrape --user-id USER_ID` â†’ Exports JSON files
2. **Upload**: Open http://localhost:3000 â†’ Click "Upload Library" â†’ Select JSON files
3. **Analyze**: View summary statistics, ratings, reading pace automatically

## Requirements

### For Scraper
- **Python 3.10 or higher** (Python 3.12+ recommended)
- Use `python3` command (not `python`)
- Install with: `python3 -m pip install -e parser/`

### For Dashboard
- **Docker Desktop** (includes Docker Compose)
  - macOS: `brew install --cask docker`
  - Linux: https://docs.docker.com/engine/install/
  - Windows: https://www.docker.com/products/docker-desktop
- **OR** for local development: Node.js 20+, PostgreSQL 15+

## Component Documentation

| Component | Status | README | Quickstart | Full Spec |
|-----------|--------|--------|------------|-----------|
| **Scraper** | âœ… Complete (MVP) | [README](./parser/README.md) | [Quickstart](./specs/001-scrape-goodreads-library/quickstart.md) | [Spec](./specs/001-scrape-goodreads-library/spec.md) |
| **Dashboard** | âœ… Phase 3 Complete | [README](./DASHBOARD.md) | [Quickstart](./specs/002-analytics-dashboard/quickstart.md) | [Spec](./specs/002-analytics-dashboard/spec.md) |

## Features

### âœ… Implemented

**Feature 001: Scrape Goodreads Library**
- Command-line scraper for Goodreads profile data
- Exports individual JSON files (one per book)
- CSV export support
- Handles pagination (up to 2000 books tested)
- Rate limiting and error handling

**Feature 002: Analytics Dashboard (MVP)**
- Web-based dashboard (React + NestJS + PostgreSQL)
- File upload (multiple JSON files from scraper)
- Summary statistics:
  - Total books by status (read, currently-reading, to-read)
  - Average rating & distribution visualization
  - Reading pace (books/month, streak)
  - Year-over-year comparison
- Session-based user tracking
- Duplicate detection
- Interactive Swagger UI at `/api/docs`

### ðŸš§ Planned (Dashboard Phases 4-6)

- **Phase 4**: Reading trends over time (line charts)
- **Phase 5**: Category breakdowns (genres, authors, decades)
- **Phase 6**: Advanced filtering & drill-down

## Development Workflow

This project uses the [SpecKit](https://github.com/anthropics/claude-code) workflow for structured feature development.

**Workflow**:
1. `/speckit.specify` - Create feature specification
2. `/speckit.plan` - Generate implementation plan
3. `/speckit.tasks` - Break down into tasks
4. `/speckit.implement` - Execute implementation

Feature specifications and implementation plans are in the `specs/` directory.

## Performance

**Scraper**:
- ~5 books/second with rate limiting
- Handles libraries up to 2000 books
- Exports ~350 books in ~70 seconds

**Dashboard**:
- Upload & parse 2000 books: **2.3s**
- Analytics API response: **180ms**
- Initial page load: **1.2s**
- Resource usage: **~270MB RAM** (3 Docker containers)

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                     â”‚
            â”‚ 1. Scrape           â”‚ 2. Upload & View
            â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scraper (Python)   â”‚  â”‚  Dashboard (Web App)   â”‚
â”‚                     â”‚  â”‚                        â”‚
â”‚  - CLI Interface    â”‚  â”‚  Frontend (React)      â”‚
â”‚  - BeautifulSoup    â”‚  â”‚  Backend (NestJS)      â”‚
â”‚  - JSON/CSV Export  â”‚  â”‚  Database (PostgreSQL) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚
           â”‚ JSON Files              â”‚ Analytics
           â–¼                         â–¼
      Export Folder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Upload & Visualize
```
